# Commits, Branch, Releases of Repository



Note: This is a Challenge Lab meaning when you finish all tasks below, click Submit button on the top and the system will inform you whether you PASS or FAIL. 

## Introduction

This challenge is based on the last challenge. We are going to add new data to the table crawled. In this challenge, database needs to include the number of commits, branch, and realeases of every repository. These information can be obtained in the repository page, for instance in the page below, you'll find these data above the codes.



![image desc](https://labex.io/upload/A/Q/J/5gpPMlX4eDAJ.png)



The page which repository list intends to crawl is: 

```
https://github.com/labexcoding?tab=repositories
```

Note there may be more than one subpage in this page. We need to crawl repository name and update time in every page, and save the results to MySQL database.

First step, start MySQL service like what we did before: 

```
sudo service mysql start
```

then log into MySQL (no password for root user), and create database. 

```
mysql > create database labexgithub;
```

You will use sqlalchemy to create database table in this challenge, and connect to batabase. At lease include these following content in the table, and save in this format: 

```
class Repository(Base):
    __tablename__ = 'repositories'
    id = Column(Integer, primary_key=True)
    name = Column(String(64))
    update_time = Column(DateTime)
    commits = Column(Integer)
    branches = Column(Integer)
    releases = Column(Integer)
```

Note that the update time information is not displayed here, but is saves in the source code of the page. In order to obtain and analyze, you need to check the page source codes in your browser.


## Objectives

1. All data crawled must be stored in the `repositories` table in `labexgithub` database
2. The detection script of this challenge only view the results, that is, whether the data in the `repositories` table of the database is accurate or not


## Tips

1. If scrapy or sqlalchemy is not installed in the challenge environment, please finish the installation as instructed
2. You can conduct page analysis in your local browser. Pay particular attention to the location of the update time in the specified format on the page
3. You can choose either CSS or XPATH as data extractor
4. Pay attention to the processing of paging. You can refer to the first lab when we use `{}` in the URL to process paging
5. You will need to use multiple pages to create an Item. Obtain repository name and  update time from here `https://github.com/labexcoding?tab=repositories` Obtain other information from here `https://github.com/labexcoding/xxxx` and then save to the same table in the database.
6. Remember to eliminate the comma when transferring to integers. Or it may appear as `2,412`
7. scrapy obey robot.txt rule of no crawling by default. It will not crawl in Github. We can switch the option to False un der settings.py

```
ROBOTSTXT_OBEY = False
```

## Outline

- Scrapy project framework
- Analayze web page element field
- SQLAlchemy define data model
- Connect to database
- Create Scrapy project
- Create scrapy
- Item container
- Item Pipeline
- Models create table
- Save Item to database
- Data composed of item are located in multiple pages


