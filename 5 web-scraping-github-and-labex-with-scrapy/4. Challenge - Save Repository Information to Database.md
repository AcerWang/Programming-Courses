# Save Repository Information to Database



Note: This is a Challenge Lab meaning when you finish all tasks below, click Submit button on the top and the system will inform you whether you PASS or FAIL. 

## Introduction

We want to acquire all Github repository lists in LabEx, and save those data to dababase. The web page we're going to crawl is: 

```
https://github.com/shiyanlou?tab=repositories
```

Please note there are 4 sub-pages. We need to crawl every repository name and update time in each page, and save the results to MySQL database.

First we need to launch MySQL based on what we've learnd in previous labs:

```
sudo service mysql start
```

Log into MySQL (no password for root user), and create database labexgithub

```
mysql > create database labexgithub;
```

In this challenge, we need to use sqlalchemy to create a data table `repositories`, and connect it to database. The table has to include the following content, save to this format: 

```
class Repository(Base):
    __tablename__ = 'repositories'
    id = Column(Integer, primary_key=True)
    name = Column(String(64))
    update_time = Column(DateTime)
```

Note the update time information is not displayed here, but is saves in the source code of the page. In order to obtain and analyze, you need to check the page source codes in your browser.

## Objectives

1. All data crawled must be stored in the `repositories` table in `labexgithub` database
2. The detection script of this challenge only view the results, that is, whether the data in the `repositories` table of the database is accurate or not

## Tips

1. If scrapy or sqlalchemy is not installed in the challenge environment, please finish the installation as instructed
2. You can conduct page analysis in your local browser. Pay particular attention to the location of the update time in the specified format on the page
3. You can choose either CSS or XPATH as data extractor
4. Pay attention to the processing of paging. You can refer to the first lab when we use `{}` in the URL to process paging
5. Scrapy obey robot.txt rule of no crawling by default. It will not crawl in Github. We can switch the option to False un der settings.py

```
ROBOTSTXT_OBEY = False
```

## Outline

- Scrapy project framework
- Analayze web page element field
- SQLAlchemy define data model
- Connect to database
- Create Scrapy project
- Create scrapy
- Item container
- Item Pipeline
- Models create tables
- Save Item to database