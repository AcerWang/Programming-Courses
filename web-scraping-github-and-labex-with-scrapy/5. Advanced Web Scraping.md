---
show: step
version: 1.0
enable_checker: true
---
# Advanced Web Scraping

## 1. Introduction

This Lab is mainly about advanced knowledge and techniques of scrapy，including downloading picture, the composition of the item data in multiple pages, and simulated login.

#### Learning Objective

- Downloading image
- Item contains multiple page data
- Simulated login

## 2. Content
- Prepare resources
- Retrive data on multiple pages
- Simulate loign
### 2.1 Download Image

Scrapy has an internally built-in pipeline which can be used for downloading pictures. We'll use LabEx course covers as examples to show you how to use this function.

First, define an item in `items.py` and this item must include 2 required fields

```python
class CourseImageItem(scrapy.Item):
    # List of pictures to download
    image_urls = scrapy.Field()
    # The downloaded picture will be placed here first
    images = scrapy.Field()
```

Execute `scrapy genspider courses_image labex.com/courses` and generate a crawler whose major work is to parse all links of pictures to image_urls of `CourseImgeItem`.

```python
# -*- coding: utf-8 -*-
import scrapy

from labex.items import CourseImageItem

class CoursesImageSpider(scrapy.Spider):
    name = 'courses_image'
    start_urls = ['https://www.labex.com/courses/']

    def parse(self, response):
        item = CourseImageItem()
        ＃ parse image link to item
        image_urls = response.css('div.course-img img:attr(src)').extract()
        image_urls = ['https://labex.io' + url for url in image_urls]
        item['image_urls'] = image_urls
        yield item
```

After the code is completed, you need to start the built-in pipeline of scrapy in settings because the pipelines in `ITEM_PIPELINES` will act on each item in sequence, and we do not need` LabexPipeline` to act on the picture item, so we remove it.


```python
ITEM_PIPELINES = {
    'scrapy.pipelines.images.ImagesPipeline': 100,
    # 'labex.pipelines.LabexPipeline': 300
}
```

You also need to configure the image store directory:

```python
IMAGES_STORE = 'images'
```

**Execution:** 

```python
# Install PIL package which are needed

pip3 install image

# Execute image to download crawler
scrapy crawl courses_image
```

Scrapy will download the picture to `images / full`, and the saving name is hash for the original file. Why there is a full directory? The full directory represents the original size of the downloaded image because scrapy can be configured to change the size of the downloaded image, such as adding the following configuration in the settings to generate a small picture：

```python
IMAGES_THUMBS = {
    'small': (50, 50)
}
```

### 2.2 The Data of the Item on Multiple Pages

In the previous crawlers we made, the data is obtained in a single page, but in an actual crawler project, we will often need to capture data from different pages to form an item. Here's an example of how to deal with this situation.

Suppose we need to crawl all course names, cover picture links and course descriptions of the home page on LabEx. We can easily have access to the course name and cover picture link in the course home page, but we can only see the course description when we click and enter the course. How do we solve this problem? 

The scrapy solution is multi-level request with parse. In other words, ask for the first course page, and parse the course name and the course picture link in the callback parse, and then construct a request to the course details page in 'parse'. And lastly, we deal with the callback function in course details page to parse the description. 

First you should create a corresponding class in items.py：

```python
class MultipageCourseItem(scrapy.Item):
    name = scrapy.Field()
    image = scrapy.Field()
    description = scrapy.Field()
```

execute `scrapy genspider multipage labex.io/courses` to generate a crawler and modify codes as follows：

```python
# -*- coding: utf-8 -*-
import scrapy
from labex.items import MultipageCourseItem

class MultipageSpider(scrapy.Spider):
    name = 'multipage'
    start_urls = ['https://labex.io/courses/']

    def parse(self, response):
        for course in response.css('div.course-item):
            item = MultipageCourseItem()
            # parse the course name
            item['name'] = course.css('span.course-title::text').extract_first()
            # parse the course image
            item['image'] = course.xpath('div.course-img img::@attr(src)').extract_first()
            # Construct a link to the course details page. The link you crawled is a relative link, and call the 'urljoin' to construct the full link
            course_url = response.urljoin(course.css('a::attr(href)').extract_first())
            # Construct a request to the course details page, specifying a callback function
            request = scrapy.Request(course_url, callback=self.parse_description)
            # pass the unfinished item into parse_description via meta
            request.meta['item'] = item
            yield request

    def parse_description(self, response):
        # acquire an unfinished item
        item = response.meta['item']
        # parse course description
        item['description'] = response.css('div.course-infobox-desc p::text').extract_first()
        # construction complete, yield item
```

Shut off all pipeline; run the crawler, and save results to files

```
scrapy crawl multipage -o data.json
```

### 2.3 Simulated Login

Some web contents are only available when you log in, such as the module on LabEx's home page

![](https://labex.io/upload/I/R/H/XVY7QVelKi9s.png)


If you want to crawl the contents that can be seen after login, you need to use scrapy to conduct a simultation then to crawl the page and parse the data. This experiment is to simulate the login home page, and then crwal the data that the arrow points to on your home page.

Normally, websites will have a login page. Login page for LabEx is https://labex.io/login. Open this page to see the source codes, and you can find the relevant codes in the login form.

![](https://labex.io/upload/W/B/O/Ocb7TbsMx1Ma.jpg)

Simulated login crawling process is similar to the multi-page crawl. The only diffirence is changing the crawl of the first page into a login form. When logging on successfully, scrapy will crawl with the returned cookie so that you can have the contents that should be crawled.

You do not need to create a project in this alb, but to create a `login_spider.py` the script below` Code`. This is the basic structure and process of the program：

```python
# -*- coding: utf-8 -*-
import scrapy

class LoginS(scrapy.Spider):
    name = 'login_spider'

    start_urls = ['https://labex.io/login']

    def parse(self, response):
        """ the core of the simulated login is that scrapy will download start_urls Lane Login page, the response will be passed here, and then call FormRequest Simulated construction of a POST login request. FormRequest is inherited from Request，
		So the parameters of the Request can apply to it. The `from_response` in FormRequest is used to quickly build a FormRequest object. from_response will obtain the requested URL, form information, etc. from the response returned in the first step.We only need to specify the necessary form data and a callback function.
        """
        return scrapy.FormRequest.from_response(
             # The first argument must be passed back to the response
             # Pass the form data into the dictionary structure
             formdata={},
             # Specifies the callback function
             callback=self.after_loin
        )

    def after_login(self, response):
        """ the code after login is the same as the ordinary scrapy crawler. Construct Request and specify callback...
        """
        pass

    def parse_after_login(self, response):
        pass
```

based on this code structure, it is easy to make mistakes：

```python
# -*- coding: utf-8 -*-
import scrapy


class LoginSpiderSpider(scrapy.Spider):
    name = 'login_spider'

    start_urls = ['https://labex.io/login']

    def parse(self, response):
        #obatin the csrf_token from the table
        csrf_token = response.xpath('//input[@name="_xsrf"]/@value').extract_first()
        self.logger.info(csrf_token)
        return scrapy.FormRequest.from_response(
            response,
            formdata={
                'csrf_token': csrf_token,
                #Here, enter your own email address and password
                'login': 'example@email.com',
                'password': 'password',
            },
            callback=self.after_login
        )

    def after_login(self, response):
        # Once the login is successful, construct a scrapy.Request that visits your homepage.
        # Remember to change the id in the URL into your own one. You can only see your data in this part.
        return [scrapy.Request(
            url='https://labex.io/user/11/',
            callback=self.parse_after_login
        )]

    def parse_after_login(self, response):
        """ The number of parsing the experiments and the data of experimental time is in the span.side-con-item structure. The number of experiments lies in the third, and the experimental time is located in the fouth.
        """
        return {
            'lab_count': response.xpath('(//span[@class="side-con-item"])[3]/text()').re_first('[^\d]*(\d*)[^\d*]'),
            'lab_minutes': response.xpath('(//span[@class="side-con-item"])[4]/text()').re_first('[^\d]*(\d*)[^\d*]')
        }
```


**Execution:**

```
scrapy runspider login_spider.py -o user.json
```
After the execution, data that could only be seen after the login is now captured and saved in the user.json.


## 3. Summary

This lab introduces how to use scrapy advanced knowledge and skills mainly through user crawler code examples of LabEx, including page follow, picture download, make up the item's data on multiple pages, simulated login and so on.
