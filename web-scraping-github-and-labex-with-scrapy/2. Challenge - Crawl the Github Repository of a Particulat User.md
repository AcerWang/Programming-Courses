# Crawl the Github Repository of a Particulat User



Note: This is a Challenge Lab meaning when you finish all tasks below, click Submit button on the top and the system will inform you whether you PASS or FAIL. 



## Introduction

We want to obtain all Github repository of a Particulat User. The intended page to crawl is: 

```
https://github.com/shiyanlou?tab=repositories
```

Note there are 4 sub-pages in this page. We need to crawl repository name and update time in every page, and save the results to the JSON file.

Data in every JSON repository include: 

```
"name": "trojan",
"update_time": "2017-06-20T11:10:55Z"
```

Note that the update time information is not displayed here, but is saves in the source code of the page. In order to obtain and analyze, you need to check the page source codes in your browser.


## Objectives

1. Put crawled JSON data in `/home/labex/labexgithub.json`
2. We only examine the results of the detection scripts, meaning to check whether data in the  `/home/labex/labexgithub.json` is accurate or not. 

## Tips

1. If scrapy or sqlalchemy is not installed in the challenge environment, please finish the installation as instructed
2. You can conduct page analysis in your local browser. Pay particular attention to the location of the update time in the specified format on the page
3. You can choose either CSS or XPATH as data extractor
4. Pay attention to the processing of paging. You can refer to the first lab when we use `{}` in the URL to process paging
5. You can use scrapy shell in the testing process.

## Outline

- Introduction to scrapy crawler framework
- Install scrapy framework
- CSS and XPATH Data extractor
- scrapy shell
- start_urls

