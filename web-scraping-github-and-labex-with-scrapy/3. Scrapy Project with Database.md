---
show: step
version: 1.0
enable_checker: true
---
# Scrapy Project with Database

## 1. Introduction

In the previous Lab, we just wrote a crawler script based on scrapy and did not use the scrapy project standard. In today's Lab, we will turn the script into the form of a standard `scrapy` project and store the crawled data in a MySQL database. We'll be using SQLAlchemy for database connection and operation.

#### Learning Objective

- Connect database
- Create Scrappy project
- Create crawler
- Item container
- Item Pipeline
- Models and table creation
- Save Item to database
- Item filter

## 2. Content
- Preparation
- Create crawler
- Scrapy item
- Use database to store data
- Item filter
### 2.1 Preparation for Connecting to Database

This lab will crawl the data stored in MySQL. We need to do some preparatory work. First, set the `MySQL` encoding format to` utf8`. Edit the configuration file:

```sh
sudo vim /etc/mysql/my.cnf
```


Add the following configuration:

```toml
[client]
default-character-set = utf8

[mysqld]
character-set-server = utf8

[mysql]
default-character-set = utf8
```


After saving, you can start mysql:

```sh
sudo service mysql start
```


Access to mysql as root; 	By default, the experimental environment has no password 
```shell
mysql -uroot
```


Create "LabEx" database for this experiment:

```sql
mysql > create database labex;
```


When you're done, enter "quit" to exit.


This experiment uses `SQLAlchemy` ORM to connect and manipulate mysql in a crawler. Install it first:

```sh
sudo pip3 install sqlalchemy
```

Also need to install Python3 driver to connect to MySQL

`mysqlclient`：

```sh
sudo apt-get install libmysqlclient-dev
sudo pip3 install mysqlclient
```

### 2.2 Create Project

Create a `scrapy` project using the` startproject` command provided by `scrapy`. We need to provide a project name, since we want to crawl the data of LabEx, so we set LabEx as the project name:


```sh
scrapy startproject labex
```

After entering "LabEx", you can see the project structure like this:

```py
Code/
    scrapy.cfg            # deploy profile
    labex/            # project name
        __init__.py
        items.py          # items definition
        pipelines.py      # project pipelines definition
        settings.py       # project profile
        spiders/          # write all crawlers in this directory:
            __init__.py
```

### 2.3 Create Crawler

The "genspider" command of "scrapy" can quickly initialize a crawler tempalte using the following method:

```sh
scrapy genspider <name> <domain>
```

`name` Crawler name
`domain` Specify the site to crawl

Go to the "labex" directory. Run the following command to quickly initialize a crawler tempalte:

```py
cd /home/labex/Code/labex/
scrapy genspider courses labex.io
```

`scrapy` creates a new` courses.py` file in the `spiders` directory and initializes the code structure in the file:

```python
# -*- coding: utf-8 -*-
import scrapy

class CoursesSpider(scrapy.Spider):
    name = 'courses'
    allowed_domains = ['labex.com']
    start_urls = ['http://labex.com/']

    def parse(self, response):
        pass
```


There is a new property `allowed_domains` which was not covered in the previous Lab. What is it? `allow_domains` can be a list or a string that contains the domain names that the crawler can crawl. Suppose the page we want to crawl is `https: // www.example.com / 1.hml`, then add` example.com` to allowed_domains. This property is optional and not rquired in our project so it can be removed.

Other than that, the code for `start_urls` is the same as the previous Lab

```python
# -*- coding: utf-8 -*-
import scrapy

class CoursesSpider(scrapy.Spider):
    name = 'courses'

    @property
    def start_urls(self):
        url = 'https://labex.io'
        yield url
```
```checker
- name: check if file exist
  script: |
    #!/bin/bash
    ls /home/labex/Code/courses.py
  error: Sorry, you didn't create file "courses.py" in /home/labex/Code!
  timeout: 2
- name: check if content exist
  script: |
    #!/bin/bash
    grep -i 'coursesspider' /home/labex/Code/courses.py
  error: Oops, you didn't create Class "CoursesSpider" in "courses.py".
  timeout: 2
```
### 2.4 Item 

The main goal of a crawler is to extract structured information from a web page. The `scrapy` crawler can return the crawled data as a Python dict, but it is not a good fit for storing structured data due to the chaotic nature of dict. Scrapy is recommended to use the `Item` container to store crawled data.


All items are written in `items.py`. Here's an` Item` defined for the course to be crawled:

```python
import scrapy

class CourseItem(scrapy.Item):
	  """It is pretty simple to define an Item. All we need to do is inherit scrapy.Item class, and declare each piece of data to crawl as scrapy.Field(). The following shows the 2 data to be crawled in the previous section.
    """
    name = scrapy.Field()
    image = scrapy.Field()
```

With CourseItem, you can wrap the return of the parse method into it:


```python
# -*- coding: utf-8 -*-
import scrapy
from labex.items import CourseItem

class CoursesSpider(scrapy.Spider):
    name = 'courses'

    @property
    def start_urls(self):
        url = 'https://labex.io'
        yield url

    def parse(self, response):
        for course in response.css('div.course-item'):
            # wrap the returned result as CourseItem; others are the same as the previous Lab
            item = CourseItem({
                'name': course.css('div.course-title::text').extract_first(),
                'image': course.css('div.course-img img::attr(src)').extract_first()
            })
            yield item
```
```checker
- name: check if file exist
  script: |
    #!/bin/bash
    ls /home/labex/Code/labex/items.py
  error: Sorry, you didn't create file "items.py" in /home/labex/Code/labex.
  timeout: 2
- name: check if content exist
  script: |
    #!/bin/bash
    grep -i 'coursesspider' /home/labex/Code/labex/piplines.py
  error: Oops, you didn't create Class "CoursesSpider" in "items.py".
  timeout: 2
```
### 2.5 Item Pipeline

If you think of `scrapy` as a product line, then ` spider` is responsible for crawling data from the webpage; `Item` is equivalent to a box, standardizing crawled data and throwing it into` Pipeline`.

In pipeline, we mainly process these several details regarding item

- Verify crawled data (check whether item has a specific field)
- Check for duplicated data
- Save to database


When creating a project, scrapy has generated a `pipline` template for the project in` piplines.py`:

```python
class LabexPipline(object):
    def process_item(self, item, spider):
        """ Item which is parsed out will be sent here. The preparation of the treatment code will be applied to each item above. This method must return an Item object.
        """
        return item
```

Other than `process_item`, here are 2 other commonly usd hooks methods - `open_spider` and ｀close_spider`：

```python
class LabexPipline(object):
    def process_item(self, item, spider):
        return item

    def open_spider(self, spider):
        """ Called when crawler is turned on
        """
        pass

    def close_spider(self, spider):
        """ Called when the crawler is turnd off
        """
        pass
```
```checker
- name: check if file exist
  script: |
    #!/bin/bash
    ls /home/labex/Code/labex/piplines.py
  error: Sorry, you didn't create file "piplines.py" in /home/labex/Code/labex.
  timeout: 2
- name: check if content exist
  script: |
    #!/bin/bash
    grep -i 'labexpipline' /home/labex/labex/piplines.py
  error: Oops, you didn't Class "LabexPipline" in "piplines.py".
  timeout: 2
```
### 2.6 Define Model & Create Table

Create `models.py` under the directory where items.py` is located and use the` sqlalchemy` syntax to define the courses table structure:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Column, String, Integer

engine = create_engine('mysql+mysqldb://root@localhost:3306/labex?charset=utf8')
Base = declarative_base()

class Course(Base):
    __tablename__ = 'courses'

    id = Column(Integer, primary_key=True)
    name = Column(String(64), index=True)
    image = Column(String(128))


if __name__ == '__main__':
    Base.metadata.create_all(engine)
```

Run the program

```py
python3 models.py
```


If you run the prgram correctly, the program will output nothing. After the implementation, go to MySQL to check whether the table has been created:

```sql
mysql > use labex;
mysql> show tables;
+-----------------+
| Tables_in_labex |
+-----------------+
| course          |
+-----------------+
```

If something similar to the above pops up then the table has been created successfully!
```checker
- name: check if file exist
  script: |
    #!/bin/bash
    ls /home/labex/Code/labex/models.py
  error: Sorry, you didn't create file "models.py" in /home/labex/Code/labex.
  timeout: 2
```
### 2.7 Save item to database

After creating a datasheet, you can write codes in the pipeline and write each item crawled into the database.

```python
from sqlalchemy.orm import sessionmaker
from labex.models import Course, engine

class LabexPipeline(object):

    def process_item(self, item, spider):
        # The number of learners extracted is a string. Convert it to int
        # Create a Course Model object based on the item and add it to the session
        # Item can be used as a dictionary, so you can use the dictionary as a destructor:
        # Course(
        #     name=item['name'],
        #     image=item['image'],
        # )
        self.session.add(Course(**item))
        return item

    def open_spider(self, spider):
        """ When crawler is turnd on, create databse session
        """
        Session = sessionmaker(bind=engine)
        self.session = Session()

    def close_spider(self, spider):
        """ When crawler is turned off, submit session and then shut off session
        """
        self.session.commit()
        self.session.close()

```

The `LabExPipeline` we wrote is off by default. To turn it on, uncomment the following code in` settings.py`:

```python
# By default, this is commented
ITEM_PIPELINES = {
    'labex.pipelines.LabexPipeline': 300
}
```


The` pipeline` which needs to be turned on inside the `ITEM_PIPELINES` configuration is a dictionary. key indicates the location of the pipeline. The value is a number, which means the execution sequence when you open multiple pipelines. Smaller value will be implementated first. The value is usually set at 100 to 1000.

### 2.8 Run

The previously used `runspider` command is used to launch a standalone scrapy crawler script. To start a crawler using the` crawl` command in a scrapy project, specify the name of the crawler:

```py
scrapy crawl courses
```

After running the crawler, enter MySQL, enter the following command to view the crawling data:

```sql
mysql> use labex;
mysql> select * from courses;
```

![](https://labex.io/upload/J/R/V/0WxHDpr3020g.png)

Because the scrapy crawler is executed asynchronously, the order of courses crawled may be different from that of LabEx site.


## 3. Summary

This Lab describes how to use the scrapy command-line tool to quickly create projects, create crawlers, and how to write crawlers and run crawlers based on project frameworks. In addition, it also describes how to use MySQL database in scrapy, and how to store the results in the database.
