---
show: step
version: 1.0
enable_checker: true
---
# Scraping Course Information of LabEx 


## 1. Introduction

`Scrapy` uses an open source crawler framework implemented by Python. With the principle of "Do not Repeat Yourself", Scrapy provides a set of solutions for preparing the basic framework for crawlers and writing common problems in the process. `Scrapy` mainly has the following functions and features：

- The built-in data extractor (Selector): supports XPath and Scrapy's own CSS Selector syntax, as well as regular expressions, which facilitates extracting information from webpage.
- Interactive command-line tool: easy testing of Selector and debugging crawlers.
- Supports exporting data to JSON, CSV, XML format.
- Numerous built-in expansions and middlewares for processing:
  - cookies and session
  - HTTP Compression, authentication, and cache
  - robots.txt
  - Deep limitation of crawlers
- Highly scalable and runs specific plug-ins written by you

In addition to these listed, there are many small features, such as built-in files, image downloader and so on. Scrapy is based on the high-performance event-driven web engine framework of Twisted, which means that Scrapy crawlers have high performance.

We're going to implement a crawler that crawls all course information in LabEx.

#### Learning Objective

- scrapy crawler framework introduction 
- scrapy framework installation
- data extractor: CSS and XPATH
- scrapy shell
- regular expression data extraction
- start_urls

## 2. Content
- Install framework
- Data extractor
- Write script
- Test
### 2.1 Installation

In order to facilitate the management of Python version and dependent package, first we need to create a python 3.5 virtual environment in the `Code` (virtualenv has been installed in experimental environment)：

```shell
virtualenv -p python3.5 venv
```

Activate the environment:

```shell
. venv/bin/activate
```


The following scrapy crawler labs are all based on this virtual environment.

install `scrapy`

```shell
pip3 install scrapy
```

Now input `scrapy` in the command line, if the following description appears then the installation is successful.

```shell
Scrapy 1.4.0 - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy <command> -h" to see more info about a command
```

From this, we can see the command line command provided by `scrapy` and its function introduction.

### 2.2 Data Extractor

Before we begin to write a crawler, let's first learn about the `scrapy` data collector. Because the essence of a crawler is to obtain data, we need to write a lot of data extraction codes in the process of writing a crawler.

`scrapy` has two built-in extration syntaxs： `CSS` and `XPath`. Let's take a look at how to use them. Here's a HTML file：

```html
<html>
 <head>
  <base href='http://example.com/' />
  <title>Example website</title>
 </head>
 <body>
  <div id='images'>
   <a href='image1.html'>Name: My image 1 <br /><img src='image1_thumb.jpg' /></a>
   <a href='image2.html'>Name: My image 2 <br /><img src='image2_thumb.jpg' /></a>
   <a href='image3.html'>Name: My image 3 <br /><img src='image3_thumb.jpg' /></a>
   <a href='image4.html'>Name: My image 4 <br /><img src='image4_thumb.jpg' /></a>
   <a href='image5.html'>Name: My image 5 <br /><img src='image5_thumb.jpg' /></a>
  </div>
 </body>
</html>
```

This is an official webpage provided by scrapy that allows us to practice `Selector`. Its address is

http://doc.scrapy.org/en/latest/_static/selectors-sample1.html

#### 2.2.1 Scrapy Shell

 `scrapy shell` provides an interactive Python environment for us to test and debug crawler, using

```shell
scrapy shell [url]
```

We need to provide a web page url. After implementation of the command, scrapy will automatically download the URL corresponding to the page, the result package as a `scrapy` internal response object and injected into the python shellcin the` response` object. You can directly use scrapy built-in css and xpath data extractor.

Run the following command to download the above page and enter:

```sh
scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html
```

#### 2.2.2 CSS Selector

：As the name suggests, css selector is the syntax of css to locate the label. For example, to extract the text of all a-tags under the div with the ID of images in the example web page, use the css syntax to write:

```python
>>> response.css('div#images a::text').extract()
['Name: My image 1 ', 'Name: My image 2 ', 'Name: My image 3 ', 'Name: My image 4 ', 'Name: My image 5 ']
```

`div # images` represents a div whose id is an image. If the class name is images, then it is` div.images`. `div a` means all the a tags in the div,` :: text` means that the text is extracted, and the `extract` function performs an extraction and returns a ** list **. If you only want the text under the first a tab in the list, you can use the `extract_first` function:

```python
>>> response.css('div#images a::text').extract_first()
'Name: My image 1 '
```

The extract_first () method supports providing a default value for elements that do not match:

```python
>>> response.css('div#images p::text').extract_first(default='default')
'default' default value
```

There is no p tag under `div#images`, therefore the default value provided is returned. None will be returned if no default value is provided.

If you want to extract all the a href links, you can write: 

```python
>>> response.css('div#images a::attr(href)').extract()
['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
```

Not just `href`, any attribute of any tag can be extracted with` attr () `. Based on the above knowledge, it is easy to write out the link to extract all the pictures of the address:

```python
>>> response.css('div#images a img::attr(src)').extract()
['image1_thumb.jpg', 'image2_thumb.jpg', 'image3_thumb.jpg', 'image4_thumb.jpg', 'image5_thumb.jpg']
```

#### 2.2.3 XPath

XPath is a path extraction language commonly used to extract information from `html / xml` files. Its basic rules are as follows:

| Expression | Description                                                  |
| ---------- | ------------------------------------------------------------ |
| nodename   | select all sub-nodes of this node                            |
| /          | select from root node                                        |
| //         | Select nodes in the document from the current node that matches the selection, regardless of their location. |
| .          | select current node                                          |
| ..         | select the parent node of the current node                   |
| @          | select property                                              |

if you want to use xpath syntax in scrapy to extract all text documents of `div#images a`, you can write the following: 

```python
>>> response.xpath('//div[@id="images"]/a/text()').extract()
['Name: My image 1 ', 'Name: My image 2 ', 'Name: My image 3 ', 'Name: My image 4 ', 'Name: My image 5 ']
```

`//` represents the current text. `Div [@ id =" images "]` represents a div with an id of images and selects a label by its properties, expressed as `tag [@ attr =" value "]`.

Extract all image links with XPath syntax:

```python
>>> response.xpath('//div[@id="images"]/a/img/@src').extract()
['image1_thumb.jpg', 'image2_thumb.jpg', 'image3_thumb.jpg', 'image4_thumb.jpg', 'image5_thumb.jpg']
```

Compared the two kinds of syntaxes, CSS syntax is relatively simple, novice friendly. CSS syntax at the bottom is actually converted to XPath. Although XPATH requires a little learning costs, but it is more powerful, and has better performance.

#### 2.2.4 re and re_first

In addition to the `extract ()` and `extract_first ()` methods, the `re ()` and `re_first ()` methods are also available for objects returned by `css ()` or `xpath ()` methods.

The content extracted directly using `extract ()` may not meet the format requirements. For example, the text of the first a tag obtained above is like this: `Name: My image 1`. Now it is not required to start with `Name:` and at the end of the space; this time you can use `re ()` instead of `extract` method. The use of regular expressions for further processing of the extracted content:

```python
>>> response.css('div#images a::text').re('Name: (.+) ')
['My image 1', 'My image 2', 'My image 3', 'My image 4', 'My image 5']
```

The regular expression defined in the `er ()` method acts on each extracted text, leaving only the contents of the regular expression matched by the subpattern, ie, matches within `()`. 

The method `re_first ()` supports only the first text:

```python
>>> response.css('div#images a::text').re_first('Name: (.+) ')
'My image 1'
```

### 2.3 Practice

Now let's use `scrapy` to write a crawler, crawling all the course names, cover image links in LabEx and save it as `JSON` file.

Create new `labex_courses_spider.py` file in `/home/labex/Code` to write the basic mechanism for `scrapy` crawlers:

```python
import scrapy

class LabexCoursesSpider(scrapy.Spider):
    """ All scrapy crawlers need to write a Spider class that inherits the scrapy. Spider class defines the website and links to be requested in this class, and how to extract data from returned web pages, and so on.
    """

    # Crawler id, scrapy project may have multiple reptiles. Name is used to identify each reptile; names cannot be the same
    name = 'labex-courses'

    def start_requests(self):
        """ We need to return an iterable object. The iterative element is the scrapy.Request object. The iterable object can be a list or an iterator, so that scrapy will know which pages need to be crawled. `scrapy.Request` accepts a url parameter and a callback parameter. url refers to the page to be crawled. Callback is a callback function used to process the returned web page, usually a parse function that extracts data.
        """

    def parse(self, response):
        """ This method is used as the callback for `scrapy.Request`, where the code to extract the data is written. The scrapy downloader downloads each `Request` defined in` start_reqeusts` and wraps the result as a response object into this method.
        """
	      pass 
```

By analyzing LabEx's course page, we can see there is only one page, the url is https://labex.io/courses

Then we'll use the `start_requests` method: 

```python
def start_requests(self):
    # the page to be crawled
    urls = 'https://labex.io/courses'
    # Returns a generator that generates a Request object; the generator is an iterable object
    for url in urls:
        yield scrapy.Request(url=url, callback=self.parse)
```

The `scrapy` internal loader downloads each` Request`, then wraps the result as a `response` object, passing in the` parse` method, which is the same object as the previous scrapy shell exercise, meaning you can Use response.css () or response.xpath () to extract the data.

For example, by analyzing the document structure of course page of LabEx and taking the "A Beginner's Guide to LabEx" as an example, the data we need to extract is mainly contained in the following div:

```html
 <div class="col-md-3 course-item">

    <a href="/alibaba/courses/2">
        <div class="course-img">
            <img src="/upload/W/L/V/BvJNo9QAhNju.png" alt="A Beginner&#39;s Guide to LabEx">
        </div>
        <div class="course-body clearfix">
            <span class="course-title">A Beginner&#39;s Guide to LabEx</span>
        </div>
        <div class="course-footer clearfix">
                        
                <span class="pull-left badge badge-purple badge-course-status updating">Updating</span>
            
        </div>
    </a>
    
</div>
```

According to this div, we can use the extractor to write parse method:

```python
def parse(self, response):
    # iter over div.course-item exists in every course
    for course in response.css('div.course-item'):
        # Use css syntax to extract data for each course
        yield {
            # Course name
            'name': course.css('span.course-title::text').extract_first(),
            # Course cover image
            'image': course.css('div.course-img img::@attr(src)').extract_first(),
        }
```
```checker
- name: check if file exist
  script: |
    #!/bin/bash
    ls /home/labex/Code/labex_courses_spider.py
  error: Sorry, you didn't create file "labex_courses_spider.py" in /home/labex/Code!
  timeout: 3
- name: check file content
  script: |
    #!/bin/bash
    grep -i 'LabexCoursesSpider' /home/labex/Code/labex_courses_spider.py
  error: Oops, you didn't create Class "LabexCoursesSpider" in "labex_courses_spider.py".
  timeout: 2
```
### 2.4 Run Scrapy

After writing the spider in the format in the previous step, you can use the `runspider` command from scrapy to run the crawler

```sh
scrapy runspider labex_courses_spider.py -o data.json
```

`-o` means that if you open a file, scrapy will serialize the result to JSON by default. After running the crawler, open the `data.json` file in the current directory, and it can crawl the data.

#### 2.4.1 start_urls

The `scrapy.Spider` class already has a default` start_requests` method. Our crawler code can be further simplified. Only the `start_urls` that need to be crawled are provided. The default` start_requests` method generates a `Request` from `start_urls` Object. So, the code can be modified as:

```python
import scrapy

class LabexCoursesSpider(scrapy.Spider):

    name = 'labex-courses'

    @property
    def start_urls(self):
        """ start_urls needs to be iterable, so you can write it as a list, a set, or a generator. Here we're using the generator
        """
        urls = 'https://labex.io/courses'
        yield 

    def parse(self, response):
    	 for course in response.css('div.course-item'):
            yield {
                'name': course.css('span.course-title::text').extract_first(),
                'image': course.css('div.course-img img::@attr(src)').extract_first()
            }
```

```checker
- name: check if file exist
  script: |
    #!/bin/bash
    ls /home/labex/Code/labex_courses_spider.py
  error: Sorry, you didn't create file "labex_courses_spider.py" in /home/labex/Code
  timeout: 3
- name: check file content
  script: |
    #!/bin/bash
    grep -i 'LabexCoursesSpider' /home/labex/Code/labex_courses_spider.py
  error: Oops, you didn't create Class "LabexCoursesSpider" in "labex_courses_spider.py".
  timeout: 2
```
## 3. Summary

This Lab describes the main features of scrapy, how to use scrapy's built-in css and xpath selectors to extract data, and how to write a simple crawler script based on scrapy, run the crawler, and save the crawl results as JSON data.
