# Concurrency Instance & Synchronization

## 1. Introduction

In this lab, we will use some exaples to strengthen your programming skills of concurrency. Goroutines and channels are simple, Go also provides more original approach to synchronization - the mutex and atomic operations. This part can be hard for novice, take it easy and read the text carefully, review knowledge of previous labs.

### Learning Objective

- goroutines and channels
- synchronization
- atomic operations
- mutex locks

## 2. Content

### 2.1 Worker Pool

In this example we'll look at how to implement a *worker pool* using goroutines and channels. Here's the worker, of which we'll run several concurrent instances. These workers will receive work on the `jobs`channel and send the corresponding results on `results`. We'll sleep a second per job to simulate an expensive task. In order to use our pool of workers we need to send them work and collect their results. We make 2 channels for this. This starts up 3 workers, initially blocked because there are no jobs yet. Here we send 5 `jobs` and then `close` that channel to indicate that's all the work we have. Finally we collect all the results of the work.

```
package main

import "fmt"
import "time"

// receive jobs and sleep for one second and send a message
func worker(id int, jobs <-chan int, results chan<- int) {
	for j := range jobs {
		fmt.Println("worker", id, "started  job", j)
		time.Sleep(time.Second)
		fmt.Println("worker", id, "finished job", j)
		results <- j * 2
	}
}

func main() {

	t := time.Now()
	// create two channel, one for receive jobs 
	// one for send message
	jobs := make(chan int, 100)
	results := make(chan int, 100)
	
	// create three workers goroutines to do work concurrently
	for w := 1; w <= 3; w++ {
		go worker(w, jobs, results)
	}
	
	// send 10 jobs in total to all workers
	for j := 1; j <= 5; j++ {
		jobs <- j
	}
	// it is safe to close channel if not use it any more
	close(jobs)
	
	// wait for all jobs done
	for a := 1; a <= 5; a++ {
		<-results
	}
	
	// calculate the total time the program consume
	d := time.Since(t)
	fmt.Println(d)
}
```

Our running program shows the 5 jobs being executed by various workers. The program only takes about 2 seconds despite doing about 5 seconds of total work because there are 3 workers operating concurrently.

```
$ time go run worker-pools.go 
worker 1 started  job 1
worker 2 started  job 2
worker 3 started  job 3
worker 1 finished job 1
worker 1 started  job 4
worker 2 finished job 2
worker 2 started  job 5
worker 3 finished job 3
worker 1 finished job 4
worker 2 finished job 5
2.0030523s
```

### 2.2 Rate Limiting

*Rate limiting* is an important mechanism for controlling resource utilization and maintaining quality of service. Go elegantly supports rate limiting with goroutines, channels, and tickers. First we'll look at basic rate limiting. Suppose we want to limit our handling of incoming requests. We'll serve these requests off a channel of the same name.

```
package main

import "time"
import "fmt"

func main() {

	// create a buffered channl 
	requests := make(chan int, 5)
	// send five numbers to the channel in a row
    for i := 1; i <= 5; i++ {
        requests <- i
    }
    // close the channel
    close(requests)
    
    //set a ticker of 1000ms
    limiter := time.Tick(1000 * time.Millisecond)
    
    // every 1000ms, iterate the channel value
    for req := range requests {
    	// this will block for one second
        <-limiter
        fmt.Println("request", req, time.Now())
    }
    
    // make another buffered channel
    burstyLimiter := make(chan time.Time, 3)
    
    // send three values to this channel
    // so the channel is full
    for i := 0; i < 3; i++ {
        burstyLimiter <- time.Now()
    }
    
    // start a new goroutine
    // every 200ms, send a value to the channel `burstyLimiter`
    // this will wait for `burstyLimiter` being received
    // and having free space for new values
    go func() {
        for t := range time.Tick(200 * time.Millisecond) {
            burstyLimiter <- t
        }
    }()
    
    // create a buffered channel
    burstyRequests := make(chan int, 5)
    // send five values to this channel
    for i := 1; i <= 5; i++ {
        burstyRequests <- i
    }
    // close this channel
    close(burstyRequests)
    
    // iterate the channel `burstyRequests`
    for req := range burstyRequests {
        <-burstyLimiter
        fmt.Println("request", req, time.Now())
    }
}
```

This `limiter` channel will receive a value every 1000 milliseconds. This is the regulator in our rate limiting scheme. By blocking on a receive from the `limiter` channel before serving each request, we limit ourselves to 1 request every 1000 milliseconds. We may want to allow short bursts of requests in our rate limiting scheme while preserving the overall rate limit. We can accomplish this by buffering our limiter channel. This `burstyLimiter` channel will allow bursts of up to 3 events. Fill up the channel to represent allowed bursting. Every 200 milliseconds we'll try to add a new value to `burstyLimiter`, up to its limit of 3. Now simulate 5 more incoming requests. The first 3 of these will benefit from the burst capability of `burstyLimiter`. Running our program we see the first batch of requests handled once every ~200 milliseconds as desired. For the second batch of requests we serve the first 3 immediately because of the burstable rate limiting, then serve the remaining 2 with ~200ms delays each.

**Output:**

```
$ go run rate-limiting.go
request 1 2018-02-28 15:23:14.2838966 +0800 CST m=+1.014019001
request 2 2018-02-28 15:23:15.2842466 +0800 CST m=+2.014369101
request 3 2018-02-28 15:23:16.2838812 +0800 CST m=+3.014003701
request 4 2018-02-28 15:23:17.2845225 +0800 CST m=+4.014645001
request 5 2018-02-28 15:23:18.2845967 +0800 CST m=+5.014719201
request 1 2018-02-28 15:23:18.2845967 +0800 CST m=+5.014719201
request 2 2018-02-28 15:23:18.2845967 +0800 CST m=+5.014719201
request 3 2018-02-28 15:23:18.2845967 +0800 CST m=+5.014719201
request 4 2018-02-28 15:23:18.4848744 +0800 CST m=+5.214996901
request 5 2018-02-28 15:23:18.6849062 +0800 CST m=+5.415028701
```

### 2.3 Atomic Counters

The primary mechanism for managing state in Go is communication over channels. We saw this for example with worker pools. There are a few other options for managing state though. Here we'll look at using the `sync/atomic` package for *atomic counters* accessed by multiple goroutines. We'll use an unsigned integer  (always-positive)  to represent our counter. To simulate concurrent updates, we'll start 50 goroutines that each increment the counter about once a millisecond. To atomically increment the counter we use `AddUint64`, giving it the memory address of our `ops` counter with the `&`syntax. Wait a bit between increments. Wait a second to allow some ops to accumulate and then end the operation. In order to safely use the counter while it's still being updated by other goroutines, we extract a copy of the current value into `opsFinal` via `LoadUint64`. As above we need to give this function the memory address `&ops` from which to fetch the value.

```
package main

import "fmt"
import "time"
import "sync/atomic"

func main() {

	var ops uint64
	// start 50 goroutines, each will do increament operation repetitively
	for i := 0; i < 50; i++ {
        go func() {
            for {
            	atomic.AddUint64(&ops, 1)
            	time.Sleep(time.Millisecond)
            }
        }()
    }
    
    time.Sleep(time.Second)
    
    // read the final result
    opsFinal := atomic.LoadUint64(&ops)
    fmt.Println("ops:", opsFinal)
}
```

**Output:**

```
$ go run atomic-counters.go
ops: 41419
```

Running the program shows that we executed about 40,000 operations. Next we'll look at mutexes, another tool for managing state.

### 2.4 Mutexes

In the previous example we saw how to manage simple counter state using atomic operations. For more complex state we can use a *mutex* to safely access data across multiple goroutines. For this example the `state` will be a map. This `mutex` will synchronize access to `state`. We'll keep track of how many read and write operations we do. Here we start 100 goroutines to execute repeated reads against the state, once per millisecond in each goroutine. For each read we pick a key to access, `Lock()` the `mutex` to ensure exclusive access to the `state`, read the value at the chosen key, `Unlock()` the mutex, and increment the `readOps` count. We'll also start 10 goroutines to simulate writes, using the same pattern we did for reads. Let the 10 goroutines work on the `state` and `mutex` for a second. Take and report final operation counts. With a final lock of `state`, show how it ended up. 

```
package main

import (
    "fmt"
    "math/rand"
    "sync"
    "sync/atomic"
    "time"
)

func main() {

	// create a map to record states
	var state = make(map[int]int)
	// create a mutex lock
	var mutex = &sync.Mutex{}
	
	// use the two variables to record operation times
	var readOps uint64
    var writeOps uint64
    
    // start 100 goroutines to do reading operation 
    for r := 0; r < 100; r++ {
        go func() {
            total := 0
            for {
            	key := rand.Intn(5)
            	
                mutex.Lock()
                // within the mutex lock, operation is goroutine-safe
                // only one goroutine can access the `state[key]`
                total += state[key]
                mutex.Unlock()
                
                // record the how many times the goroutine access state variable
                atomic.AddUint64(&readOps, 1)
                time.Sleep(time.Millisecond)
            }
        }()
    }
    
    // start 10 goroutines to do write operation
    for w := 0; w < 10; w++ {
        go func() {
            for {
                key := rand.Intn(5)
                val := rand.Intn(100)
                
                // within the mutex lock, operation is goroutine-safe
                // only one goroutine can set value for the `state[key]`
                mutex.Lock()
                state[key] = val
                mutex.Unlock()
                
                // record the how many times the goroutine set state variable
                atomic.AddUint64(&writeOps, 1)
                time.Sleep(time.Millisecond)
            }
        }()
    }
    
    time.Sleep(time.Second)
    
    readOpsFinal := atomic.LoadUint64(&readOps)
    fmt.Println("readOps:", readOpsFinal)
    writeOpsFinal := atomic.LoadUint64(&writeOps)
    fmt.Println("writeOps:", writeOpsFinal)
    
    // as goroutines are still running
    // in order to get the right result
    // use mutex lock to access `state`
    mutex.Lock()
    fmt.Println("state:", state)
    mutex.Unlock()
}
```

Running the program shows that we executed about 90,000 total operations against our `mutex`-synchronized `state`.

**Output:**

```
$ go run mutexes.go
readOps: 83285
writeOps: 8320
state: map[1:97 4:53 0:33 2:15 3:2]
```

Next we'll look at implementing this same state management task using only goroutines and channels.

### 2.5 Stateful Goroutines

In the previous example we used explicit locking with mutexes to synchronize access to shared state across multiple goroutines. Another option is to use the built-in synchronization features of goroutines and channels to achieve the same result. This channel-based approach aligns with Go's ideas of *sharing memory by communicating* and having each piece of data owned by exactly 1 goroutine. In this example, our state will be owned by a single goroutine. This will guarantee that the data is never corrupted with concurrent access. In order to read or write that state, other goroutines will send messages to the owning goroutine and receive corresponding replies. 

These `readOp` and `writeOp` structs encapsulate those requests and a way for the owning goroutine to respond. As before we'll count how many operations we perform. The `reads` and `writes` channels will be used by other goroutines to issue read and write requests, respectively.

Here is the goroutine that owns the `state`, which is a map as in the previous example but now private to the stateful goroutine. This goroutine repeatedly selects on the `reads`and `writes` channels, responding to requests as they arrive. A response is executed by first performing the requested operation and then sending a value on the response channel `resp` to indicate success (and the desired value in the case of `reads`).

This starts 100 goroutines to issue reads to the state-owning goroutine via the `reads` channel. Each read requires constructing a `readOp`, sending it over the `reads`channel, and the receiving the result over the provided `resp` channel. We start 10 writes as well, using a similar approach.

Let the goroutines work for a second. Finally, capture and report the op counts.

```
package main

import (
    "fmt"
    "math/rand"
    "sync/atomic"
    "time"
)

type readOp struct {
    key  int
    resp chan int
}
type writeOp struct {
    key  int
    val  int
    resp chan bool
}

func main() {
	
	// use the two variables to record operation times
	var readOps uint64
    var writeOps uint64
    
    // create two channel for reading and writing state
    reads := make(chan *readOp)
    writes := make(chan *writeOp)
    
    // in this goroutine, read state or write state again and again
    go func() {
    	// this state variable is owned by single goroutine 
        var state = make(map[int]int)
        for {
            select {
                case read := <-reads:
                    read.resp <- state[read.key]
                case write := <-writes:
                    state[write.key] = write.val
                    write.resp <- true
            }
        }
    }()
    
    // start 100 goroutines to read state
    for r := 0; r < 100; r++ {
        go func() {
            for {
                read := &readOp{
                    key:  rand.Intn(5),
                    resp: make(chan int)}
                reads <- read
                <-read.resp
                atomic.AddUint64(&readOps, 1)
                time.Sleep(time.Millisecond)
            }
        }()
    }
    
    // start 10 goroutines to write state
    for w := 0; w < 10; w++ {
        go func() {
            for {
                write := &writeOp{
                    key:  rand.Intn(5),
                    val:  rand.Intn(100),
                    resp: make(chan bool)}
                writes <- write
                <-write.resp
                atomic.AddUint64(&writeOps, 1)
                time.Sleep(time.Millisecond)
            }
        }()
    }
    
    time.Sleep(time.Second)
    
    readOpsFinal := atomic.LoadUint64(&readOps)
    fmt.Println("readOps:", readOpsFinal)
    writeOpsFinal := atomic.LoadUint64(&writeOps)
    fmt.Println("writeOps:", writeOpsFinal)
}
```

Running our program shows that the goroutine-based state management example completes about 80,000 total operations. For this particular case the goroutine-based approach was a bit more involved than the mutex-based one. It might be useful in certain cases though, for example where you have other channels involved or when managing multiple such mutexes would be error-prone. You should use whichever approach feels most natural, especially with respect to understanding the correctness of your program.

**Output:**

```
$ go run stateful-goroutines.go
readOps: 71708
writeOps: 7177
```

## 3. Summary

State management in multithread programming is always a complex part. In golang, you can use advanced approach of goroutines and channels or use low-level mutexes. The Go's philosophy: sharing memory by communicating rather than communicating by sharing memory. Take time to make out the example of *stateful goroutines*.