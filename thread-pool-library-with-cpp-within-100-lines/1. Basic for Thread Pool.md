---
show: step
version: 1.0
enable_checker: true
---
# Basic for Thread Pool

## 1. Introduction

In server-side development, we usually facing the tasks of scheduling and managing the execution performance of process. In this project, we will implement a simple thread pool library by using C++11.

#### Things to Learn

- C++11 Library Features
  - std::thread
  - std::mutex, std::unique_lock
  - std::condition_variable
  - std::future, std::packaged_task
  - std::function, std::bind
  - std::shared_ptr, std::make_shared
  - std::move, std::forward

- C++11 Language Features
  - Lambda Expression
  - Trail Return Type

- Thread Pool Model
- Test Driven

**To Readers**

This project is an advanced project even the code has only lower than 100 lines. Please be much more patient for it.

### 2.1 What is a Thead Pool

Multi-threading technology mainly focuses on processing multiple thread in a single processor. Thus we have the threading pool technology. Thread pool is composited by:

1. Thread pool manager: for creating, managing thread pool. The basic operation is: Construct, Destruct, and Push new tasks;
2. Worker: Working process in thread pool, it execute idle process if thread pool has no task;
3. Tasks Queue: Unprocessed tasks in queue cache.

In short, a thread pool is responsible for managing the number of threads that need to be executed for multiple concurrent executions, and the scheduling between them.

For a deeper understanding of the thread pool technology, let's look at an actual example.

In a web server, if the server needs to process one million requests in a day, and each request needs to be completed by a separate thread. In order to ensure the efficient execution of server tasks (excute as fast as possible), the number of concurrent execution threads should not be increased unchecked, so the thread pool plays a role in this.

Considering that a processor completes a task is divided into three phases: creating a thread, executing a thread, and destroying a thread.

If there are 100,000 concurrent requests occurring during a peak period of access, and the request for each task is very simple, even if the time for executing the task is less than the time the thread was created, then the processor must spend a lot of time. To create these request threads, but for a long time let each thread not be executed.

With the thread pool, we can create a certain number of threads after the program starts. When the task arrives, the buffer queue will add the task to the thread to execute. After the execution is completed, the thread does not destroy but wait for the next task is arrived.


![image desc](https://labex.io/upload/V/W/E/UqiD0QioMtnm.png)


### 2.2 Basics about C++11

C++11 introduces very rich and useful new features, especially the large number of new features supported by concurrent programming, which makes it possible to write a complex thread pool within 100 lines. Before designing the thread pool, let's review the features we might use.

> We will review (or learn) the following features of C++11, generic programming, and concurrent models (mutex locks) in multiple threads. **If you are familiar with these, you can skip directly and check out the next document**:
>
> 1. Language Features
> - lambda expression
> - Tail return type
> - Rvalue reference
>
> 2. Standard Library Features
> - std::thread
> - std::mutex, std::unique_lock
> - std::future, std::packaged_task
> - std::condition_variable
> - std::function, std::bind
> - std::shared_ptr, std::make_shared
> - std::move, std::forward

### 2.3 Language Features

#### 2.3.1 Lambda Expression

Lambda expression is one of the most important new features in C++11. Lambda expressions actually provide a feature that is similar to anonymous functions. An anonymous function requires a function, but you don't want to laboriously name it. The use of a function case continues. There are many, many scenarios like this, so anonymous functions are almost standard in modern programming languages.

The basic syntax for Lambda expression is as follows:

```
[capture_list](parameter_list) mutable(optional) exception_property -> return_type {
    // function body
}
```

The above grammar rules are well understood except the contents of `[capture_list]`, the function name is omitted and the return value is in the form of a `->`.

The so-called capture list, in fact, can be understood as a type of parameter, lambda expression internal function body by default is not able to use the function body external variables, capture list at this time can play a role in the transfer of external data. According to the transmitted behavior, the capture list is also divided into the following categories:

**Value Capture**

Similar to parameter passing, the previous stage of value capture is that the variable can be copied. The difference is that the captured variable is copied when the lambda expression is created, instead of being copied when invoked:

```cpp
void learn_lambda_func_1() {
    int value_1 = 1;
    auto copy_value_1 = [value_1] {
        return value_1;
    };
    value_1 = 100;
    auto stored_value_1 = copy_value_1();
    // In this case, stored_value_1 == 1, but value_1 == 100.
    // due to copy_value_1 stored a copy when construct value_1
}
```

**Reference Capture**

Similar to reference pass arguments, reference captures hold references, and values change.

```cpp
void learn_lambda_func_2() {
    int value_2 = 1;
    auto copy_value_2 = [&value_2] {
        return value_2;
    };
    value_2 = 100;
    auto stored_value_2 = copy_value_2();
    // In this case stored_value_2 == 100, value_1 == 100.
    // because of copy_value_2 stores reference
}
```

**Implicit Capture**

Writing a capture list manually is sometimes very complicated. This kind of mechanical work can be handed to the compiler for processing. At this time, you can write a `&` or `=` in the capture list to declare a reference capture or value to the compiler. capture.

To summarize, capturing provides the ability to use lambda expressions for external values. The four most commonly used forms for capturing a list can be:

- [] Empty capture list
- [name1, name2, ...] capture a set of variables
- [&] Reference capture, compiler deduces capture list
- [=] Value capture, compiler deduces capture list

#### 2.3.2 Trail Return Type

Sometimes, when you want to write a function to receive an application that returns an element in a sequence container, you may not be able to understand how to write the function's return value type:

```
template <typename T>
return_type &getItem(T begin, T end) {
    return *begin; // return the reference of the first element
}
```

How can we write the `return_type` here? In fact, we might think of using `decltype()` to fetch the type, but when the compiler reads this function definition, `begin` doesn't even show up, and we have no way to directly write the return type at this point.

C++11 provides a new way to write return values, which is to put the return type at tail. The tail return type allows us to declare the returned type after the argument list. Our code can be written as:

```cpp
template <typename T>
auto &getItem(T begin, T end) -> decltype(*begin) {
    return *begin; // returns the reference of the first element
}
```

where we use `decltype` to tell the compiler that the return type is as same as the return type in the parameter list, and decltype is automatically inferred as a reference to the element type.

Of course, it is not the only situation that can use the tail return type. Any function can do this. The advantage of this method is that it can make our return types clear, so that we won't feel complicated for return type, for example:

```cpp
int (*)[5]func(int value) {
}
```

can be:

```cpp
auto func(int value) -> int (*)[5] {
}
```

### 2.4 Standard Library Feature

Let's review the new features for concurrency programming in C++11.

#### 2.4.1 std::thread

`std::thread` is used to create an executing thread instance, so it is the basis for all concurrent programming. The headfile `<thread>` provides a lot of basic thread operations, such as `get_id()` for getting thread ID of the created thread, for example, use `join()` to join a thread, etc. For example:

```cpp
#include <iostream>
#include <thread>
void foo() {
    std::cout << "hello world" << std::endl;
}
int main() {
    std::thread t(foo);
    t.join();
    return 0;
}
```

#### 2.4.2 std::mutex, std::unique_lock

We have learned about the basics of concurrent technology in the relevant knowledge of the operating system, mutex is one of the core technology there. C++11 introduces mutex-related classes, and all of whose related functions are placed in the `<mutex>` header file.

`std::mutex` is the most basic `mutex` class in C++11. By instantiating `std::mutex` it is possible to create mutexes, which can only be enabled by its member function `lock()` Locks, `unlock()` can be used for unlock for sure. However, in the process of actually writing code, it is better not to directly call the member function, because calling the member function requires calling `unlock()` at the exit of each critical section, and of course, exceptions. At this time, C++11 also provides a RAIL syntax template class `std::lock_gurad` for the mutex. RAII guarantees the exceptional security of the code without losing the simplicity of the code.

Under RAII usage, the creation of a mutex for a critical section only needs to be at the beginning of the scope, for example:

```cpp
void some_operation(const std::string &message) {
    static std::mutex mutex;
    std::lock_guard<std::mutex> lock(mutex);

    // ...operations

    // when this block finished, mutex will be deconstructed and unlock.
    // Thus, this section can be seen as a critical section
}
```

Since C++ guarantees all stack objects can be destroyed at the end of the declaration period, thus the code is also exceptionally safe. Whether `some_operation()` returns normally or if an exception is thrown midway, a stack rollback is raised and `unlock()` is automatically called.

While `std::unique_lock` is more relevant to `std::lock_guard`, `std::unique_lock` is more flexible, and the object of `std::unique_lock` will have exclusive ownership (there is no other `unique_lock` object can also have ownership of a `mutex` object) to manages the locking and unlocking operations on `mutex` objects. So in concurrent programming, `std::unique_lock` is recommended. E.g:

```cpp
#include <iostream>
#include <thread>
#include <mutex>

std::mutex mtx;

void block_area() {
    std::unique_lock<std::mutex> lock(mtx);
    //...critical section
}
int main() {
    std::thread thd1(block_area);

    thd1.join();

    return 0;
}
```

#### 2.4.3 std::future, std::packaged_task

`std::future` provides a way to access the result of an asynchronous operation. This sentence is very difficult to understand. To understand this feature, we need to understand the multithreading behavior before C++11 standard.

Imagine if our main thread A wishes to open a new thread B to perform some of our expected tasks and return me a result. At this time, thread A may be busy with other things, taking care of B's demerits, so we would naturally like to be able to obtain the results of thread B at a specific time.

Before C++11's `std::future` was introduced, it was common practice to create a thread A, start task B in thread A, send an event when ready, and save the result in a global variable. . While the main function thread A is doing other things, when a result is needed, a thread is called waiting for a function to get the result of the execution.

The `std::future` provided by C++11 simplifies this process and can be used to obtain the results of asynchronous tasks. Naturally, we can easily imagine using it as a simple thread synchronization method.

In addition, `std::packaged_task` can be used to encapsulate any target that can be invoked for asynchronous calls. For instance:

```cpp
#include <iostream>
#include <future>
#include <thread>
 
int main()
{
    // encapsulate a lambda expression to task
    // the template parameter of std::packaged_task is the type of encapsulated function
    std::packaged_task<int()> task([](){return 7;});
    // get the future of task
    std::future<int> result = task.get_future();    // execute task in a thread
    std::thread(std::move(task)).detach();    std::cout << "Waiting...";
    result.wait();
    // outputs
    std::cout << "Done!" << std:: endl << "Result is " << result.get() << '\n';
}
```

After encapsulating the target you want to the call, you can use `get_future()` to get a `std::future` object to synchronize the fact thread later.

#### 2.4.4 std::condition_variable

`std::condition_variable` is created to solve a deadlock, it is introduced when the mutex operation is not enough. For example, a thread may need to wait for a condition to be true to continue execution, and a deadlock may happen when a busy wait loop may cause all other threads to fail to enter the critical region and make the condition true. Therefore, the `condition_variable` instance is created to appear primarily to wake up waiting threads to avoid deadlocks. `notify_one()` for `std::condition_variable` is used to wake a thread; `notify_all()` is used to notify all threads. The following is an example of a producer and consumer model:

```cpp
#include <condition_variable>
#include <mutex>
#include <thread>
#include <iostream>
#include <queue>
#include <chrono>

int main()
{
    std::queue<int> produced_nums;
    std::mutex m;
    std::condition_variable cond_var;
    bool done = false;
    bool notified = false;

    // producer process
    std::thread producer([&]() {
        for (int i = 0; i < 5; ++i) {
            std::this_thread::sleep_for(std::chrono::seconds(1));
            // create mutex
            std::unique_lock<std::mutex> lock(m);
            std::cout << "producing " << i << '\n';
            produced_nums.push(i);
            notified = true;
            // notify a thread
            cond_var.notify_one();
        }
        done = true;
        cond_var.notify_one();
    });

    // consumer process
    std::thread consumer([&]() {
        std::unique_lock<std::mutex> lock(m);
        while (!done) {
            while (!notified) {  // Avoid false awakening
                cond_var.wait(lock);
            }
            while (!produced_nums.empty()) {
                std::cout << "consuming " << produced_nums.front() << '\n';
                produced_nums.pop();
            }
            notified = false;
        }
    });

    producer.join();
    consumer.join();
}
```

#### 2.4.5 std::function, std::bind

`std::function` is a generic, polymorphic function wrapper whose instances can store, copy, and invoke any target entity that can be invoked. It is also a safety type of encapsulating exist callable entity in C++ (relatively, function pointer calls are not type-safe), in other words, it is a function container.

The `std::bind` is used to bind the parameters of the function call. When we use `std::packaged_task` to wrap an execution function, it is necessary to pass the call parameters, then you can use `std::packaged_task`. `std::bind` binds an argument to the calling function. For instance:

```cpp
#include <iostream>
#include <functional>

int Foo(int a, int b, int c) {
    ;
}
int main() {
    // Bind parameter 1, 2 to function Foo, and use std::placeholders::_1 as placeholder for the first parameter
    auto bindFoo = std::bind(Foo, std::placeholders::_1, 1,2);
    // call bindFoo , only need provide the first parameter
    bindFoo(1);
}
```

#### 2.4.6 std::shared_ptr, std::make_shared

C++11 also made many improvements in memory management, one of which is `std::make_shared`. It comes with `std::shared_ptr`, which is a smart pointer that can record how many `shared_ptrs` point to a common object (which is familiar to Objective-C. This feature is called reference counting. ), can eliminate the display call `delete`, when the reference count becomes 0 will automatically delete the object.

But that's not enough, because using `std::shared_ptr` still needs to be called with `new`, which makes the code somewhat asymmetrical. Therefore, another method (factory mode) is needed to solve this problem.

`std::make_shared` can be used to eliminate the use of `new`, so `std::make_shared` assigns the object that created the passed argument and returns the `std::shared_ptr` pointer to this object type. E.g:

```cpp
#include <iostream>
#include <memory>
 
void foo(std::shared_ptr<int> i)
{
    (*i)++;
}
int main()
{
    // created a std::shared_ptr
    auto pointer = std::make_shared<int>(10);
    foo(pointer);
    std::cout << *pointer << std::endl;
}
```

#### 2.4.7 std::move, std::forward, std::result_of

`std::move` and `std::forward` seem to be used to transfer something from the name, but in reality they don't transfer anything. At runtime, they don't generate a single line of code. Their function is to convert line types. And `std::move` will unconditionally convert its own argument to the right one. For example:

```cpp
#include <iostream> // std::cout
#include <utility>  // std::move
#include <vector>   // std::vector
#include <string>   // std::string

int main() {

    std::string str = "Hello world.";
    std::vector<std::string> v;

    // use push_back(const T&), copy behavior
    v.push_back(str);
    // output: "str: Hello world."
    std::cout << "str: " << str << std::endl;

    // use push_back(const T&&), rvalue reference, no copy
    // move entire string to vectorï¼Œstd::move use for reducing expensive copy
    // str will be empty after this
    v.push_back(std::move(str));
    // output: "str: "
    std::cout << "str: " << str << std::endl;

    return 0;
}
```

The `std::forward` is just as its name implies. It's behavior is very similar to `std::move`, except that it converts a parameter to a right value and converts it to a right value.

For `std::result_of`, its effect is to derive the return value type of a function call expression at compile time, for example:

```cpp
struct S {
    double operator()(char, int&); // return double
};
 
int main()
{

    std::result_of<S(char, int&)>::type foo = 3.14; // deduce return type
    typedef std::result_of<S(char, int&)>::type MyType; // is it type of double?
    std::cout << "foo's type is double: " << std::is_same<double, MyType>::value << std::endl;
    return 0;
}
```

The final output will be:

```
foo's type is double: true
```

## 3. Summary

In this experiment, we reviewd massive C++11 features. Those are our foundation knowledge for writing the library of thread pool. In the next experiment, we will using the following features for C++11:

1. lambda expression
2. std::thread
3. std::mutex, std::unique_lock
4. std::condition_variable
5. std::future, std::packaged_task
6. std::function, std::bind
7. std::shared_ptr, std::make_shared
8. std::move, std::forward

## 4. Reference

1. [Thread pool - wikipedia](https://en.wikipedia.org/wiki/Thread_pool)
2. [RAII - wikipedia](https://en.wikipedia.org/wiki/RAII)
3. [Monitor (Synchronization) - wikipedia](https://en.wikipedia.org/wiki/Monitor_%28synchronization%29)
4. [C++ Standard Library Reference](http://en.cppreference.com/w/Main_Page)
5. [C++ Concurrency in Action](https://www.manning.com/books/c-plus-plus-concurrency-in-action)
