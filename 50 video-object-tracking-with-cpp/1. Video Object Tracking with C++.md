---
show: step
version: 1.0
enable_checker: true
---
# Video Object Tracking with C++

## 1. Introduction

In this project, we will implement video object tracking by using OpenCV.
You must finish the course "Implementing Solar System with C++" before learning this project.

#### Things to Learn

- C++ Basics
- g++ Basics
- Images representation
- OpenCV Application
- Meanshift & Camshift Algorithm

#### Final  Results

This experiment will implement a program that tracking the planet in a solar system (In the following image, we selected the Saturn from gree orbit, and you can see the tracking object has been marked by a red eclipse):


![image desc](https://labex.io/upload/S/F/L/mGJk25sZD4sI.png)

Before you write this project, you must finish our course "Building Solar System with C++".

### 2.1 Creating Video File

In LabEx environment, we haven't support camera environment. Thus, we need create a video file for our project.

Let's install video recording tool:

```bash
sudo apt-get update && sudo apt-get install gtk-recordmydesktop
```

After installation, we can find the recording software in the application menu:

![image desc](https://labex.io/upload/V/M/R/m4jhDNBN9JVQ.png)

Then, you can run the solar system program `./solarsystem` and use RecordMyDesktop to record the desktop screen (10~30s will be fine), and save it to `~/Code/camshift` with name `video`:


![image desc](https://labex.io/upload/C/P/O/sAuHEZizSefx.png)


When you want to finish recording, you can click the stop button in the bottom right corner, and then you will get a `video.ogv` file:


![image desc](https://labex.io/upload/U/S/U/7y6Qpp4S9lkG.png)



### 2.2 Basics of Digital Image

OpenCV is an open source cross-platform computer vision library. Unlike OpenGL's image rendering, OpenCV implements many common algorithms for image processing and computer vision. Before learning OpenCV, we need to understand some basic concepts of images and videos in the computer.

First of all, we must understand how the picture is represented in the computer: the image exists in a continuously changing during its display process, but in a computer, there are two common ways of storing pictures: one is vector map, and the other is pixel map.

In vector map, images are mathematically defined as a series of points connected by lines. The graphic element in a vector map file is called an object. Each object is a self-contained entity, which has properties such as color, shape, outline, size, and screen position.

The more common one is the pixel map. For example, the size of an image is often 1024*768. This means that the picture has 1024 pixels in the horizontal direction and 768 pixels in the vertical direction.

The pixel is the basic unit of the pixel map. Usually, a pixel is a mixture of three primary colors (red, green and blue). Since the nature of the computer is the recognition of numbers, under normal circumstances we represent a primary color in terms of brightness from 0 to 255. In other words, for primary red colors, 0 means the darkest, black, and 255 means brightest, i.e. pure red.

Thus, a pixel can be represented as a triple `(B,G,R)`, such as white can be represented as `(255,255,255)`, and black is `(0,0,0)`, then we also We call this image an image in **RGB color space**. R, G, B become the three **channels** of the image, and there are many other color spaces besides the RGB color space, such as HSV, YCrCb, and so on.

The pixel is the basic unit of the pixel map, and the image is the basic unit of the video. A video consists of a series of images in which we call the image as a frame. And what we usually call video frame rate means that this video contains many frame images per second. For example, if the frame rate is 25, then this video will play 25 frames per second.

If there are 1000 milliseconds in 1 second, let say the rate is `rate` then the time interval between images in each frame is `1000/rate`.

### 2.3 Color Histogram of Image

A color histogram is a tool for describing an image. It is similar to a normal histogram, except that the color histogram needs to be calculated from a certain image.

If a picture is in a RGB color space, then we can count the number of occurrences of each color in the R channel. Thus we can get an array of 256 lengths (color probability lookup table). Divide all the values simultaneously by the total number of pixels (width times height) in the image and convert the resulting sequence into a histogram. The result is a color histogram of R channel. In similar way, you can have the histogram in G channel and B channel.

### 2.4 Back Project of Histogram

t has been proved that in the RGB color space, it is sensitive to changes in light illumination. In order to reduce the impact of this change on the tracking effect, the histogram needs to be back-projected. This is divided into three steps:

1. Firstly, we convert the image from RGB space to HSV space.
2. Then we calculate the histogram of the H channel.
3. The value of each pixel in the image is replaced with the corresponding probability in the color probability look-up table to obtain a color probability distribution map.

This process is called back projection and the color probability distribution map is a grayscale image.

### 2.5 Basics of OpenCV

We need install OpenCV first:

```bash
sudo apt-get install libopencv-dev
```

We assume you already know the basic syntax of C++, you know that almost every program will using the head file `#inclucde <iostream>` and `using namespace std;` or `std::cout`. OpenCV has its own namespace too.

To use OpenCV, we only need include the following head file:

```c++
#include <opencv2/opencv.hpp>
```

Then:

```c++
using namespace cv;
```

to enable OpenCV namespace (or directly using `cv::` prefix for all API)

This is your first time of using OpenCV, and you shall unfimiliar with OpenCV interfaces, thus, we recommends using `cv::` prefix for learning OpenCV APIs.

Let's write our first program to read our recorded video:

```c++
//
// main.cpp
//
#include <opencv2/opencv.hpp> // OpenCV head file

int main() {

    // create a video capsure object
    // OpenCV offers VideoCapture object and 
    // treat reading video from file as same as reading from camera.
    // when input parameter is a file path, it will read a video file;
    // if it is a identifier number of camera (usually it is 0), 
    // it will read the camera
    cv::VideoCapture video("video.ogv"); // reading from file
    // cv::VideoCapture video(0);        // reading from camera

    // container for the reading image frame, Mat object in OpenCV
    // The key class in OpenCV is Mat, which means Matrix
    // OpenCV use matrix to describe images
    cv::Mat frame;
    while(true) {

        // write video data to frame, >> is overwrited by OpenCV
        video >> frame;

        // when there is no frame, break the loop
        if(frame.empty()) break;

        // visualize current frame
        cv::imshow("test", frame);

        // video frame rate is 15, so we need wait 1000/15 for playing smoothly
        // waitKey(int delay) is a waiting function in OpenCV
        // at this point, the program will wait `delay` milsec for keyboard input
        int key = cv::waitKey(1000/15);

        // break the loop when click ECS button on keyboard
        if (key == 27) break;
    }
    // release memory
    cv::destroyAllWindows();
    video.release();
    return 0;

}
```

put this `main.cpp` file in the same folder with `video.gov` at `~/Code/camshift`, and compile the program:

```
g++ main.cpp `pkg-config opencv --libs --cflags opencv` -o  main
```

run the program, we can see the video is playing:

```
./main
```


![image desc](https://labex.io/upload/F/X/E/XLQ4oX8aQ9fi.png)


> **Note**
>
> You may observe the following error:
>
> ```
> libdc1394 error: Failed to initialize libdc1394
> ```
>
> This is a bug from OpenCV, it doesn't influence our running.
>
> If you want to eliminate the problem, you can run the following code before running the program:
>
> ```
> sudo ln /dev/null /dev/raw1394
> ```

## 3. Meanshift and Camshift Algorithm
- Meanshift
- Camshift
- Set the mouse callback event to select the tracking target
- Read the image from the video stream
- Implement the Camshift

### 3.1 Meanshift

The Meanshift and Camshift algorithms are two classic algorithms for object tracking. Camshift is based on Meanshift. Their mathematical interpretation is complex, but the basic idea is relatively simple. So we skip those mathematical facts and first introduce the Meanshift algorithm.

Assuming that there is a red set of dots on the screen, the blue circle (window) must be moved to the points there is the most dense region (or where the points are most numerous):


![image desc](https://labex.io/upload/N/B/B/gwY2uYjAh5ya.jpg)


As shown in above image, mark the blue circle as `C1` and the center of the circle as `C1_o`. But the barycenter of this circle is `C1_r`, marked as a blue solid circle.

When `C1_o` and `C1_r` do not overlap, move the circle `C1` to the center of the circle `C1_r` repeatedly. Eventually it will stay on the highest density circle `C2`.

For image processing, we usually use the back projection histogram of the image. When the tracking target moves, it is clear that this movement process can be reflected by the back-projection histogram. So the Meanshift algorithm will eventually move our selected window to the position of the moving target (algorithm has proved convergence in the end).

### 3.2 Camshift

After the previous description, we saw that the Meanshift algorithm always tracks a fixed window size, which is not in line with our needs, because in a video, the target object does not have to be large.

So Camshift was created to improve this problem. This can also be seen from the Continuously Adaptive Meanshift of Camshift.

Its basic idea is: First apply the Meanshift algorithm. Once the Meanshift results converge, Camshift updates the window size, calculates a directional ellipse to match the window, and then applies the ellipse as a new window to apply the Meanshift algorithm.

OpenCV provides a generic interface to the Camshift algorithm:

```c++
RotatedRect CamShift(InputArray probImage, Rect& window, TermCriteria criteria)
```

The first parameter, `probImage`, is the back projection of the target histogram. The second parameter, `window`, is the search window of the Camshift algorithm. The third parameter is the condition of the end of the algorithm.

### 3.3 Analysis

After understanding the basic idea of the Camshift algorithm, we can analyze the implementation of this code is mainly divided into several steps:

1. Set the mouse callback event to select the tracking target;
2. Read the image from the video stream;
3. Implement the Camshift process;

Below we continue to modify the code in `main.cpp`。

### Step 1: Select Tracking Object by Mouse Callback Function

OpenCV is differ from OpenGL, there are five parameters specified for the mouse callback function. The first three are what we need most: Through the value of `event`, we can get the event of pressed left button of the mouse (`CV_EVENT_LBUTTONDOWN`), and the event of release of left button (`CV_EVENT_LBUTTONUP`) and so on.

```c++
bool selectObject = false; // use for whether selected object or not
int trackObject = 0;       // 1 means has a tracking object, 0 means no object, and -1 means haven't calculated the Camshift property
cv::Rect selection;        // save selected region by mouse
cv::Mat image;             // cache image from video

// Callback function of mouse from OpenCV:
// void onMouse(int event, int x, int y, int flag, void *param)
// the fouth parameter `flag` represents additional state, 
// param means user parameter, we don't need them, so, no names.
void onMouse( int event, int x, int y, int, void* ) {
    static cv::Point origin;
    if(selectObject) {
        // determing selected height and width and top-left corner position
        selection.x = MIN(x, origin.x);
        selection.y = MIN(y, origin.y);
        selection.width = std::abs(x - origin.x);
        selection.height = std::abs(y - origin.y);
        
        // & is overwrited by cv::Rect
        // it means the intersection of two region, 
        // the main purpose here is to process the region outside selected region
        selection &= cv::Rect(0, 0, image.cols, image.rows);
    }
    
    switch(event) {
            // processing left button is pressed
        case CV_EVENT_LBUTTONDOWN:
            origin = cv::Point(x, y);
            selection = cv::Rect(x, y, 0, 0);
            selectObject = true;
            break;
            // processing left button is released
        case CV_EVENT_LBUTTONUP:
            selectObject = false;
            if( selection.width > 0 && selection.height > 0 )
                trackObject = -1; // tracking object haven't calculate Camshift property
            break;
    }
}
```

### Step 2: Reading Image from video streaming

We has implemented the structure of reading video streaming, let's write more details:

```c++
int main() {
    cv::VideoCapture video("video.ogv");
    cv::namedWindow("CamShift at LabEx");

    // 1. register mouse event callback
    cv::setMouseCallback("CamShift at LabEx", onMouse, NULL);

    cv::Mat frame;

    // 2. read image from video
    while(true) {
        video >> frame;
        if(frame.empty()) break;

        // write image from frame to global variable image for cache
        frame.copyTo(image);

        // draw ractangle if selecting object
        if( selectObject && selection.width > 0 && selection.height > 0 ) {
            cv::Mat roi(image, selection);
            bitwise_not(roi, roi);
        }
        imshow("CamShift at LabEx", image);
        int key = cv::waitKey(1000/15.0);
        if(key == 27) break;
    }
    // release allocated memory
    cv::destroyAllWindows();
    video.release();
    return 0;
}
```

> **Note:**
>
> ROI (Region of Interest), in image processing, any region to be processed can be a region of interest, namely ROI.

### Step 3: Implementing Camshift with OpenCV

The back projection histogram for calculating the tracking target needs to use the `cvtColor` function, which can convert the original image of the RGB color space to the HSV color space. Calculating the histogram must be after selecting the initial target, therefore:

```cpp
int main() {
    cv::VideoCapture video("video.ogv");
    cv::namedWindow("CamShift at LabEx");
    cv::setMouseCallback("CamShift at LabEx", onMouse, NULL);
    
    cv::Mat frame;
    cv::Mat hsv, hue, mask, hist, backproj;
    cv::Rect trackWindow;             // tracking window
    int hsize = 16;                   // for histogram
    float hranges[] = {0,180};        // for histogram
    const float* phranges = hranges;  // for histogram
    
    while(true) {
        video >> frame;
        if(frame.empty()) break;
        frame.copyTo(image);
        
        // trasfer to HSV space
        cv::cvtColor(image, hsv, cv::COLOR_BGR2HSV);
        // processing when there is an object
        if(trackObject) {
            
            // only processing H: 0~180，S: 30~256，V: 10~256，filter the others and copy the rest part to mask
            cv::inRange(hsv, cv::Scalar(0, 30, 10), cv::Scalar(180, 256, 10), mask);
            // seperate channel h from hsv
            int ch[] = {0, 0};
            hue.create(hsv.size(), hsv.depth());
            cv::mixChannels(&hsv, 1, &hue, 1, ch, 1);
            
            // property extract if tracking object haven't been calculated
            if( trackObject < 0 ) {
                
                // setup channel h and mask ROI
                cv::Mat roi(hue, selection), maskroi(mask, selection);
                // calculate ROI histogram
                calcHist(&roi, 1, 0, maskroi, hist, 1, &hsize, &phranges);
                // normalization of histogram
                normalize(hist, hist, 0, 255, CV_MINMAX);
                
                // setting tracking object 设置追踪的窗口
                trackWindow = selection;
                
                // mark tracking object has been calculated
                trackObject = 1;
            }
            // back project histogram
            calcBackProject(&hue, 1, 0, hist, backproj, &phranges);
            // fetch common region 
            backproj &= mask;
            // call Camshift algorithm
            cv::RotatedRect trackBox = CamShift(backproj, trackWindow, cv::TermCriteria( CV_TERMCRIT_EPS | CV_TERMCRIT_ITER, 10, 1 ));
            // processing region is too small for draw
            if( trackWindow.area() <= 1 ) {
                int cols = backproj.cols, rows = backproj.rows, r = (MIN(cols, rows) + 5)/6;
                trackWindow = cv::Rect(trackWindow.x - r, trackWindow.y - r,
                                       trackWindow.x + r, trackWindow.y + r) & cv::Rect(0, 0, cols, rows);
            }
            // draw tracking area
            ellipse( image, trackBox, cv::Scalar(0,0,255), 3, CV_AA );
            
        }
        
        
        if( selectObject && selection.width > 0 && selection.height > 0 ) {
            cv::Mat roi(image, selection);
            bitwise_not(roi, roi);
        }
        imshow("CamShift at LabEx", image);
        int key = cv::waitKey(1000/15.0);
        if(key == 27) break;
    }
    cv::destroyAllWindows();
    video.release();
    return 0;
}
```

```checker
- name: check if file exist
  script: |
    #!/bin/bash
    ls /home/labex/Code/main.cpp
  error: Sorry, you didn't create file "main.cpp" in /home/labex/Code!
  timeout: 3
```

## 4. Summary

The following shows all we wrote in this project:

```cpp
#include <opencv2/opencv.hpp>

bool selectObject = false; // use for whether selected object or not
int trackObject = 0;       // 1 means has a tracking object, 0 means no object, and -1 means haven't calculated the Camshift property
cv::Rect selection;        // save selected region by mouse
cv::Mat image;             // cache image from video

// Callback function of mouse from OpenCV:
// void onMouse(int event, int x, int y, int flag, void *param)
// the fouth parameter `flag` represents additional state, 
// param means user parameter, we don't need them, so, no names.
void onMouse( int event, int x, int y, int, void* ) {
    static cv::Point origin;
    if(selectObject) {
        // determing selected height and width and top-left corner position
        selection.x = MIN(x, origin.x);
        selection.y = MIN(y, origin.y);
        selection.width = std::abs(x - origin.x);
        selection.height = std::abs(y - origin.y);
        
        // & is overwrited by cv::Rect
        // it means the intersection of two region, 
        // the main purpose here is to process the region outside selected region
        selection &= cv::Rect(0, 0, image.cols, image.rows);
    }
    
    switch(event) {
            // processing left button is pressed
        case CV_EVENT_LBUTTONDOWN:
            origin = cv::Point(x, y);
            selection = cv::Rect(x, y, 0, 0);
            selectObject = true;
            break;
            // processing left button is released
        case CV_EVENT_LBUTTONUP:
            selectObject = false;
            if( selection.width > 0 && selection.height > 0 )
                trackObject = -1; // tracking object haven't calculate Camshift property
            break;
    }
}

int main( int argc, const char** argv ) {
    cv::VideoCapture video("video.ogv");
    cv::namedWindow("CamShift at LabEx");
    cv::setMouseCallback("CamShift at LabEx", onMouse, NULL);

    cv::Mat frame, hsv, hue, mask, hist, backproj;
    cv::Rect trackWindow;             // tracking window
    int hsize = 16;                   // for histogram
    float hranges[] = {0,180};        // for histogram
    const float* phranges = hranges;  // for histogram
    
    while(true) {
        video >> frame;
        if(frame.empty()) break;
        frame.copyTo(image);
        
        // trasfer to HSV space
        cv::cvtColor(image, hsv, cv::COLOR_BGR2HSV);
        // processing when there is an object
        if(trackObject) {
            
            // only processing H: 0~180，S: 30~256，V: 10~256，filter the others and copy the rest part to mask
            cv::inRange(hsv, cv::Scalar(0, 30, 10), cv::Scalar(180, 256, 256), mask);
            // seperate channel h from hsv
            int ch[] = {0, 0};
            hue.create(hsv.size(), hsv.depth());
            cv::mixChannels(&hsv, 1, &hue, 1, ch, 1);
            
            // property extract if tracking object haven't been calculated
            if( trackObject < 0 ) {
                
                // setup channel h and mask ROI
                cv::Mat roi(hue, selection), maskroi(mask, selection);
                // calculate ROI histogram
                calcHist(&roi, 1, 0, maskroi, hist, 1, &hsize, &phranges);
                // normalization of histogram
                normalize(hist, hist, 0, 255, CV_MINMAX);
                
                // setting tracking object
                trackWindow = selection;
                
                // mark tracking object has been calculated
                trackObject = 1;
            }
            // back project histogram
            calcBackProject(&hue, 1, 0, hist, backproj, &phranges);
            // fetch common region 
            backproj &= mask;
            // call Camshift algorithm
            cv::RotatedRect trackBox = CamShift(backproj, trackWindow, cv::TermCriteria( CV_TERMCRIT_EPS | CV_TERMCRIT_ITER, 10, 1 ));
            // processing region is too small for draw
            if( trackWindow.area() <= 1 ) {
                int cols = backproj.cols, rows = backproj.rows, r = (MIN(cols, rows) + 5)/6;
                trackWindow = cv::Rect(trackWindow.x - r, trackWindow.y - r,
                                       trackWindow.x + r, trackWindow.y + r) & cv::Rect(0, 0, cols, rows);
            }
            // draw tracking area
            ellipse( image, trackBox, cv::Scalar(0,0,255), 3, CV_AA );
            
        }
        
        
        if( selectObject && selection.width > 0 && selection.height > 0 ) {
            cv::Mat roi(image, selection);
            bitwise_not(roi, roi);
        }
        imshow("CamShift at LabEx", image);
        int key = cv::waitKey(1000/15.0);
        if(key == 27) break;
    }
    cv::destroyAllWindows();
    video.release();
    return 0;
}
```

Let's re-compile `main.cpp`:

```bash
g++ main.cpp `pkg-config opencv --libs --cflags opencv` -o main
```

and run:

```bash
./main
```

Now, we can select object in the program, and the tracking is on the way:

![image desc](https://labex.io/upload/E/E/B/Uwh4Vsy3ScIE.png)

> In the above image, we selected Saturn, and the tracking window is a red ellipse.

## 5. Reference

1. OpenCV Tutorial. [http://docs.opencv.org/2.4/](http://docs.opencv.org/2.4/).
2. Learning OpenCV. [http://shop.oreilly.com/product/0636920044765.do](http://shop.oreilly.com/product/0636920044765.do)
3. Gary, Bradsky. Computer Vision Face Tracking for Use in a Perceptual User Interface. [http://opencv.jp/opencv-1.0.0_org/docs/papers/camshift.pdf](http://opencv.jp/opencv-1.0.0_org/docs/papers/camshift.pdf)
